
# Redis 性能排查

## 确定是否是 Redis 导致变慢

1. 可以使用 Cat、SkyWalking 这类 APM 工具，链路追踪，分析是哪一环调用时间超时、慢等情况
2. 业务服务器与 Redis 服务器之间存在网络问题，比如网络线路质量不佳，网络数据包在传输时存在延迟、丢包等
3. Redis 本身问题，进一步排查，例如 __基准测试__

#### 基准测试

在生产服务器上测试 Redis 的最大相应延迟和平均响应延迟

1. 测试实例 60s 内的最大响应延迟

   ```
   redis-cli -h 127.0.0.1 -p 6379 --intrinsic-latency 60
   ```

2. 查看一段时间（间隔）内 Redis 的最大、最小、平均延迟

   ```
   redis-cli -h 127.0.0.1 -p 6379 --lantency-history -i 1
   ```

## 使用复杂度过高的命令

可以先打开慢日志 （slowlog），记录了哪些命令统计耗时超出预定值

```
# 命令执行超过5毫秒
CONFIG SET slowlog_slower_than 5000
# 记录的最大数目
CONFIG SET slowlog_max_len 1000
# 查询最近10条慢日志
SLOWLOG GET 10
```

#### latency latest

通过配置 `config set latency-monitor-threshold 1000` 设置命令执行时长的阈值， 然后可以通过 `latency latest`  查询延迟情况

Redis 命令超时特点：

1. 使用 O(n) 以上的复杂度的命令，例如 SORT、SUNION、ZUNIONSTORE 等聚合类命令
2. 使用 O(n) 复杂度命令，SMEMBERS 但 N 的值非常大，会导致收集的数据量比较多，响应慢

由于 Redis 是单线程处理执行命令，如果正在处理的命令请求慢，会导致后面请求排队，整体访问性能下降，即如果有 CPU耗时的命令，建议不在 Redis 上进行计算，而是返回客户端后在客户端上完成数据计算（聚合、整理）。

## Big Key

Redis 在写入数据时，需要为新的数据分配内存，相对应的，当从 Redis 中删除数据时，释放对应的内存空间。

如果一个 key 写入的 value 非常大，那么在分配内存时会比较耗时，同样，在删除这个 key 时，释放内存对象也会增加耗时

```
# 查看 bigkey 分布情况
redis-cli -h 127.0.0.1 -p 6379 --bigkeys -i 0.01

（线上慎用）本质上是使用 scan 命令遍历查找 keys，根据各个 key 的类型进行统计
1. 线上使用 bigkey 扫描的时候，Redis OPS 突增，为了减小性能损耗，-i 执行扫描间隔
2. 扫描的结果对于(list、set、zset、hash) 只是代表元素数量多，并不是内存占用，需要具体情况具体分析
```

1. 业务层面应该避免使用写入 bigkey
2. Redis 4.0 可以使用 __UNLINK__ 代替 DEL，降释放内存的操作放到后台线程
3. Redis 6.0 可以开启 `lazy-free` 机制（lazyfree-lazy-user-del = true），执行 DEL 的操作会被放入到后台线程

## 集中过期

特点：在某个时间点突然出现一波延时，Redis 访问变慢时间点很有规律，例如（整点、每隔一段时间）出现一波延迟

#### 过期数据删除策略

1. 被动删除：当访问某个 key 的时候才会去检查，如果过期则删除
2. 主动删除：Redis 内部维护了一个定时任务，每隔100ms（1s 10次）从全局过期 ht 中随机取出20个 key，然后删除过期的key，每次删除完成后检测过期 key 比例是否超过了 25%，如果超过重复此过程，如果本次删除任务超过 25ms，才会退出循环。

解决办法：

1. 集中过期 key 增加一个随机过期时间，把集中过期的时间打散，降低 Redis 清理过期 key 的压力
2. Redis 4.0+ 启动 `lazy-free` ，删除时会将操作放入后台线程执行

## 实例内存上限

__allkeys-lru、volatile-lru__ ：每次从实例中随机取出一批 key，然后淘汰一个最少访问的 key，之后把剩下的 key 暂存到一个池子中，继续随机取一批 key，并与之前的 key 比较，继续淘汰，反复这个过程，直至实例的内存低于 maxmemory 以下。

优化：

1. 还是避免大 key，大 key 的释放在这个过程会很耗时
2. 拆分实例，将淘汰的 key 压力分摊在多个实例上
3. lazy-free

## fork

fork：主进程创建子进程的过程中，需要拷贝自己的内存页表给子进程，如果实例很大，会放发这个过程

```
# 查看上次 fork 时间点
info

lastest_fork_usec:
```

优化：

1. 控制实例内存：10G以下，实例越大，约有可能因为节省内存执行 aof rewrite 造成 fork，也有可能在新从节点连接的时候产生 RDB 造成 fork
2. 持久化策略：在 slave 节点执行 RDB 备份（流量少时执行），非敏感数据可以关闭 AOF 和 AOF REWRITE
3. 降低主从库全量同步的概率：适当调大 `repl-blacklog-size` 参数，避免主从全量同步

## AOF

appendfsnyc everysec：（主线程和子线程双写盘）如果后台线程由于负载过高，导致 fsync 发生阻塞，迟迟不能返回，那么主线程在执行 write 系统调用时，也会被阻塞住，直到后台线程 fsync 执行完成后，主线程执行
write 才能成功。（AOF 赶上了 AOF rewrite）

优化：

1. 开启 `no-appendfsync-on-rewrite = yes`，即在 aof rewrite 期间，将刷盘策略改成了 appendfsync none，虽然避免了磁盘的阻塞，但在 aof rewrite 期键 可能会 __
   丢失数据__

### swap

内存swap 是操作系统里将内存数据在内存和磁盘间来回换入和换出的机制，一旦触发 swap 机制，无论是被换入数据的进程还是被换出的数据进程， 其性能都会受到慢磁盘读写的影响。对于 Redis 这种内存数据库来说，Redis
的请求操作需要等到磁盘数据读写完成才行，将会阻塞主 IO线程。

触发 swap 的原因：物理机器内存数据不足

* Redis 实例本身占用了大量的内存，导致物理机器的可用内存不足

* Redis 实例同一机器上的其他进程在进行大量的 __文件读写操作__，这些操作会占用系统内存，增大了触发 swap 的概率。

### 内存大页

Linux 2.6.38 开始支持大页机制，支持 2MB 大小的内存页分配，而常规内存页分配按照 4KB 大小。

因为 Redis 持久话过程中的会采用写时复制机制(RDB fork子进程)，一旦数据要更改，Redis 并不会直接修改内存中的数据， 而是将数据拷贝一份再进行修改。如果开启了内存大页机制，尽管客户端请求只修改了 100B
的数据，却要为其分配 2MB 的大页， 导致 Redis 性能变慢。

`cat /sys/kernel/mm/transparent_hugepage/enabled`

[波动的延迟响应](https://time.geekbang.org/column/article/287819)

### cpu

在多 CPU 架构下，一个应用程序访问所在 Socket 的本地内存和访问远端内存的延迟并不一致，应用程序在 多CPU 架构下，在 Socket 上调度时， 需要通过 Socket内存进行访问，__远端内存访问__
会增加应用程序的延迟。这个架构称为非统一内存访问架构（Non-Uniform Memory Access）NUMA架构。

多核CPU 和 多CPU 架构对比:

* L1、L2缓存中的指令和数据访问速度很快，所以充分利用L1、L2缓存，可以有效缩短应用程序的执行时间
* 在 NUMA 架构下，如果一个应用程序从一个 socket 调度到

context switch 是指线程的上下文切换，上下文是线程的运行时数据。在 CPU 多核的环境中，一个线程先在一个 CPU 核上运行，然后切换到另一个 CPU核上 运行，这时就会发生 context switch。context
switch 发生后，Redis 主线程的运行时信息需要被重新加载到另一个 CPU核上，此时，另一个 CPU 核上 的L1、L2 缓存上，并没有 Redis 实例之前运行频繁访问的指令核数据。所以这些数据都需要通过 L3
缓存、甚至是内存中加载。加载需要一定时间，且 Redis 实例 需要等待这个重新加载完成之后，才开始处理请求。

#### CPU 多核

* 逻辑核：在主流的 CPU 处理器中，每个物理核通常会运行两个超线程，也称为逻辑核，同个物理核的多个逻辑核会共享 L1、L2缓存
* 物理核：一个 CPU 处理器中一般有多个运行核心，我们把一个运行核心称为一个物理核，每个物理核可以单独运行应用程序，一个 CPU上多个物理核共享L3缓存

在 CPU 多核场景下，Redis 实例被频繁调度到不同的 CPU 核上运行的话，那么对 Redis 实例的请求处理时间影响更大。每调度一次，一些请求就会受到
运行时信息、指令和数据重新加载过程的影响，这就会导致某些请求的延迟明显高于其他请求。

可以利用绑核 `taskset -c 0,12 ./redis-server`，将 Redis 实例绑定在 0,12 号核上(两个逻辑核位于同一个物理核上)。

但当我们把 Redis 实例绑定到一个 CPU 逻辑核上时，就会导致子进程、后台线程和Redis主线程竞争 CPU 资源，一旦子进程或者后台线程占用 CPU时，
主线程就会阻塞，导致 Redis 请求延迟增加。所以应该绑在同一个物理核上，发挥多个核心工作，同时充分利用L1、L2缓存指令数据。

#### 多 CPU 架构

为了提升 Redis 的网络性能，把操作系统的网络中断处理程序和 CPU核绑定，这样可以避免中断处理程序在不同核上来回调度执行，能够有效
提升 Redis 的网络处理性能。

在 CPU 的 NUMA 架构下，当网络中断程序和 Redis 实例各自所绑定的 CPU核 不在同一个 CPU Socket 上，那么，Redis实例读取网络数据时，
就需要跨 CPU Socket 访问内存，会花费比较多时间。

[为什么CPU结构也会影响Redis的性能？](https://time.geekbang.org/column/article/286082)


### scan

* 漏key

    Redis 在 Scan 遍历全局哈希表时，采用 __高位加一__ 的方式遍历哈希桶，当哈希表扩容后，旧哈希表中的数据映射到新哈希表，
    依旧会保留原来的顺序，保证遍历过程中不会遗漏，也不重复。
    
* 重复key

    哈希表缩容。已经遍历过的哈希桶在缩容时，会映射到新哈希表没有遍历到的位置，所以继续遍历就会对同一个key返回多次。

[美团针对Redis Rehash机制的探索和实践](https://tech.meituan.com/2018/07/27/redis-rehash-practice-optimization.html)


### 内存碎片

* 内因：内存分配器的分配策略

    内存分配器一般是按照固定的大小分配内存空间，而不是按照应用程序申请多少就分配多少，所以在分配的过程中，会偏大以便修改数据内存时可以用
    额外空间存储数据。

    Redis 使用了 jemalloc 进行分配，按照一系列固定大小的划分内存空间，比如8B、16B...8KB等，比如 Redis 申请一个20B的空间保存数据，
    那么 jemalloc 就会为其分配32B，如果再写入时，就可以利用这个空间，而不用额外分配操作。

* 外因：键值对大小不一样和删除操作

    删除操作或者修改数据（改小），导致连续的内存空间，部分出现内存碎片

#### info memory

`mem_fragmentation_ratio = used_memory_rss / used_memory`

* used_memory_rss：操作系统实际分配给 Redis 的物理内存空间  
* used_memory：Redis保存数据实际申请使用的空间
* mem_fragmentation_ratio：当前的内存碎片率，一般时 1-1.5，如果超过1.5，表明内存碎片已经超过内存的一半了

注意：如果 `mem_fragmentation_ratio` 小于1，那么分配给它的小于实际存储的，说明发生了 swap，没有足够的物理内存给 Redis，导致 Redis
一部分数据 swap 到了磁盘中，延迟增大，性能下降。

#### 清理内存碎片

`config set activedefrag yes` 开启 Redis 的自动清理策略，清理的本质是将部分数据复制到内存碎片上，并释放原来的内存空间，因为
Redis 是单线程执行，所以清理策略要考略执行的频率（清理时机）和执行的时间点（CPU）

* `active-defrag-ignore-bytes 100mb`：表示内存碎片字节数达到100MB，开始清理
* `active-defrag-thresthod-low 10`：表示内存碎片空间占系统分配给 Redis 的总空间比例达到10%，开始清理

* `active-defrag-cycle-min 25`：自动清理过程所用的 CPU 时间的比例不低于 25%，保证清理能正常工作
* `active-defrag-cycle-max 75`：自动清理过程所用 CPU 时间不高于 75%，一旦超过，停止清理，从而避免在清理时，大量拷贝内存阻塞 Redis，影响延迟

[内存占用清理](https://time.geekbang.org/column/article/289140)


### 缓冲区溢出

为了避免客户端和服务端的请求发送和处理速度不匹配，服务端给每个连接的客户端都设置了一个输入缓冲区和输出缓冲区。输入缓冲区存储客户端发送
过来的命令，等待主线程处理。处理完成之后，返回的结果将写入到输出缓冲区中，等会返回给客户端。

* 客户端输入缓冲区溢出

    1. 写入了 bigKey，默认存储大小为1G，超过嫁给你溢出
    2. 服务端处理命令请求过慢（Redis 主线程阻塞，导致无法正常处理命令，缓冲区命令堆积）
    
* 客户端输出缓冲区溢出

    1. 服务端返回 bigKey 查询的结果
    2. 执行 MONITOR 命令（持续输出监测到的命令操作，并输出到缓冲区，生产环境禁用）
    3. 缓冲区设置太小（比如大key，或者 PubSub 情况，订阅消息过多占用大量输出缓冲区）
    
    
* 复制缓冲区溢出

    全量复制过程中，主节点向从节点发送 RDB 文件的同时，会继续接受客户端的写命令，这些写命令就会保存在复制缓冲区，等 RDB 文件传输完成后，
    再发送给从节点执行，主节点会为每个从节点维护一个复制缓冲区，来保证主从节点间的数据同步。如果在这期键，从节点加载 RDB 速度慢，同时
    主节点接受大量写命令，就会发生复制缓冲器溢出。
    
    `config set client-output-buffer-limit slave 512mb 128mb 60`
    
* 复制积压缓冲区

    以上三者本质上都属于客户端缓存，位于 Redis客户端和服务端之间或者是主从之间，`client-output-buffer-limit` 一旦溢出连接将会关闭。

    `repl_blacklog_buffer` 有限的环形缓冲区，挤压缓冲区写满后，会覆盖缓冲区中的旧命令，如果从节点还没同步，那么从节点会重新开始全量复制。

    主节点在把接收到的写命令同步给从节点时，同时会把这些写命令写入复制积压缓冲区。一旦从节点发生网络闪断，再次和主节点恢复连接后， 从节点就会从复制积压缓冲区中，读取断连期间主节点接收到的写命令，进而进行增量同步


1. 命令发送过大：避免 bigKey，对于复制缓冲区来说，避免较大的 RDB 文件
2. 命令处理慢：减少 Redis 主线程上的阻塞操作，例如异步删除
3. 空间过小：合理设置缓冲区大小

其实缓冲区就是用来匹配两端发送处理请求速度不匹配的情况，而且通过buffer可以实现批量发送，减少了网络IO带来的损耗。

[缓冲区](https://time.geekbang.org/column/article/291277)

### 缓存

计算机系统中，默认有两种缓存：

* CPU 里面的末级缓存 LLC，用来存储内存中的数据，避免每次从内存中读取数据
* 内存中的高速页缓存，即 Page Cache，用来缓存磁盘中的数据，避免每次从磁盘中读取存取的数据

#### 读写缓存策略

1. 只读缓存：如果要修改数据，先去数据库中修改，然后再删除缓存中的数据，等到下个访问这个数据时，从数据库读完并写回到缓存中。优点是，数据库和缓存
   可以保证完全一直，并且缓存中永远保留的是经常访问的热点数据。缺点是每次修改操作都会把缓存中的数据删除，之后都会触发一次缓存缺失，然后从数据库加载到 缓存中，访问延迟回增大。

2. 读写缓存：同时修改数据库和缓存中的值。优点：被修改的数据永远在缓存中，下次访问时能直接命中缓存数据，性能较好，比较适合先修改然后立即访问的 业务场景，但在高并发下，存在多个操作同时修改同一个值的情况，可能会导致缓存和数据库的不一致。

3.只读缓存如果修改数据库失败，缓存中的数据也不会删除，此时数据库和缓存中的数据一致。（如果修改成功，但删除缓存失败，那么只能消息队列重试保证最终一致性） 而如果先修改缓存，然后再修改数据库，缓存成功，但数据库失败，
就会产生数据不一致。而如果先数据库，再修改缓存，会产生并发场景不一致的情况。
