
# Redis 性能排查

## 确定是否是 Redis 导致变慢

1. 可以使用 Cat、SkyWalking 这类 APM 工具，链路追踪，分析是哪一环调用时间超时、慢等情况
2. 业务服务器与 Redis 服务器之间存在网络问题，比如网络线路质量不佳，网络数据包在传输时存在延迟、丢包等
3. Redis 本身问题，进一步排查，例如 __基准测试__

#### 基准测试

在生产服务器上测试 Redis 的最大相应延迟和平均响应延迟

1. 测试实例 60s 内的最大响应延迟

   ```
   redis-cli -h 127.0.0.1 -p 6379 --intrinsic-latency 60
   ```

2. 查看一段时间（间隔）内 Redis 的最大、最小、平均延迟

   ```
   redis-cli -h 127.0.0.1 -p 6379 --lantency-history -i 1
   ```

## 使用复杂度过高的命令

可以先打开慢日志 （slowlog），记录了哪些命令统计耗时超出预定值

```
# 命令执行超过5毫秒
CONFIG SET slowlog_slower_than 5000
# 记录的最大数目
CONFIG SET slowlog_max_len 1000
# 查询最近10条慢日志
SLOWLOG GET 10
```

#### latency latest

通过配置 `config set latency-monitor-threshold 1000` 设置命令执行时长的阈值， 然后可以通过 `latency latest`  查询延迟情况

Redis 命令超时特点：

1. 使用 O(n) 以上的复杂度的命令，例如 SORT、SUNION、ZUNIONSTORE 等聚合类命令
2. 使用 O(n) 复杂度命令，SMEMBERS 但 N 的值非常大，会导致收集的数据量比较多，响应慢

由于 Redis 是单线程处理执行命令，如果正在处理的命令请求慢，会导致后面请求排队，整体访问性能下降，即如果有 CPU耗时的命令，建议不在 Redis 上进行计算，而是返回客户端后在客户端上完成数据计算（聚合、整理）。

## Big Key

Redis 在写入数据时，需要为新的数据分配内存，相对应的，当从 Redis 中删除数据时，释放对应的内存空间。

如果一个 key 写入的 value 非常大，那么在分配内存时会比较耗时，同样，在删除这个 key 时，释放内存对象也会增加耗时

```
# 查看 bigkey 分布情况
redis-cli -h 127.0.0.1 -p 6379 --bigkeys -i 0.01

（线上慎用）本质上是使用 scan 命令遍历查找 keys，根据各个 key 的类型进行统计
1. 线上使用 bigkey 扫描的时候，Redis OPS 突增，为了减小性能损耗，-i 执行扫描间隔
2. 扫描的结果对于(list、set、zset、hash) 只是代表元素数量多，并不是内存占用，需要具体情况具体分析
```

1. 业务层面应该避免使用写入 bigkey
2. Redis 4.0 可以使用 __UNLINK__ 代替 DEL，降释放内存的操作放到后台线程
3. Redis 6.0 可以开启 `lazy-free` 机制（lazyfree-lazy-user-del = true），执行 DEL 的操作会被放入到后台线程

## 集中过期

特点：在某个时间点突然出现一波延时，Redis 访问变慢时间点很有规律，例如（整点、每隔一段时间）出现一波延迟

#### 过期数据删除策略

1. 被动删除：当访问某个 key 的时候才会去检查，如果过期则删除
2. 主动删除：Redis 内部维护了一个定时任务，每隔100ms（1s 10次）从全局过期 ht 中随机取出20个 key，然后删除过期的key，每次删除完成后检测过期 key 比例是否超过了 25%，如果超过重复此过程，如果本次删除任务超过 25ms，才会退出循环。

解决办法：

1. 集中过期 key 增加一个随机过期时间，把集中过期的时间打散，降低 Redis 清理过期 key 的压力
2. Redis 4.0+ 启动 `lazy-free` ，删除时会将操作放入后台线程执行

## 实例内存上限

__allkeys-lru、volatile-lru__ ：每次从实例中随机取出一批 key，然后淘汰一个最少访问的 key，之后把剩下的 key 暂存到一个池子中，继续随机取一批 key，并与之前的 key 比较，继续淘汰，反复这个过程，直至实例的内存低于 maxmemory 以下。

优化：

1. 还是避免大 key，大 key 的释放在这个过程会很耗时
2. 拆分实例，将淘汰的 key 压力分摊在多个实例上
3. lazy-free

## fork

fork：主进程创建子进程的过程中，需要拷贝自己的内存页表给子进程，如果实例很大，会放发这个过程

```
# 查看上次 fork 时间点
info

lastest_fork_usec:
```

优化：

1. 控制实例内存：10G以下，实例越大，约有可能因为节省内存执行 aof rewrite 造成 fork，也有可能在新从节点连接的时候产生 RDB 造成 fork
2. 持久化策略：在 slave 节点执行 RDB 备份（流量少时执行），非敏感数据可以关闭 AOF 和 AOF REWRITE
3. 降低主从库全量同步的概率：适当调大 `repl-blacklog-size` 参数，避免主从全量同步

## AOF

appendfsnyc everysec：（主线程和子线程双写盘）如果后台线程由于负载过高，导致 fsync 发生阻塞，迟迟不能返回，那么主线程在执行 write 系统调用时，也会被阻塞住，直到后台线程 fsync 执行完成后，主线程执行
write 才能成功。（AOF 赶上了 AOF rewrite）

优化：

1. 开启 `no-appendfsync-on-rewrite = yes`，即在 aof rewrite 期间，将刷盘策略改成了 appendfsync none，虽然避免了磁盘的阻塞，但在 aof rewrite 期键 可能会 __
   丢失数据__

### swap

内存swap 是操作系统里将内存数据在内存和磁盘间来回换入和换出的机制，一旦触发 swap 机制，无论是被换入数据的进程还是被换出的数据进程， 其性能都会受到慢磁盘读写的影响。对于 Redis 这种内存数据库来说，Redis
的请求操作需要等到磁盘数据读写完成才行，将会阻塞主 IO线程。

触发 swap 的原因：物理机器内存数据不足

* Redis 实例本身占用了大量的内存，导致物理机器的可用内存不足

* Redis 实例同一机器上的其他进程在进行大量的 __文件读写操作__，这些操作会占用系统内存，增大了触发 swap 的概率。

### 内存大页

Linux 2.6.38 开始支持大页机制，支持 2MB 大小的内存页分配，而常规内存页分配按照 4KB 大小。

因为 Redis 持久话过程中的会采用写时复制机制(RDB fork子进程)，一旦数据要更改，Redis 并不会直接修改内存中的数据， 而是将数据拷贝一份再进行修改。如果开启了内存大页机制，尽管客户端请求只修改了 100B
的数据，却要为其分配 2MB 的大页， 导致 Redis 性能变慢。

`cat /sys/kernel/mm/transparent_hugepage/enabled`

[波动的延迟响应](https://time.geekbang.org/column/article/287819)

### cpu

在多 CPU 架构下，一个应用程序访问所在 Socket 的本地内存和访问远端内存的延迟并不一致，应用程序在 多CPU 架构下，在 Socket 上调度时， 需要通过 Socket内存进行访问，__远端内存访问__
会增加应用程序的延迟。这个架构称为非统一内存访问架构（Non-Uniform Memory Access）NUMA架构。

多核CPU 和 多CPU 架构对比:

* L1、L2缓存中的指令和数据访问速度很快，所以充分利用L1、L2缓存，可以有效缩短应用程序的执行时间
* 在 NUMA 架构下，如果一个应用程序从一个 socket 调度到

context switch 是指线程的上下文切换，上下文是线程的运行时数据。在 CPU 多核的环境中，一个线程先在一个 CPU 核上运行，然后切换到另一个 CPU核上 运行，这时就会发生 context switch。context
switch 发生后，Redis 主线程的运行时信息需要被重新加载到另一个 CPU核上，此时，另一个 CPU 核上 的L1、L2 缓存上，并没有 Redis 实例之前运行频繁访问的指令核数据。所以这些数据都需要通过 L3
缓存、甚至是内存中加载。加载需要一定时间，且 Redis 实例 需要等待这个重新加载完成之后，才开始处理请求。

#### CPU 多核

* 逻辑核：在主流的 CPU 处理器中，每个物理核通常会运行两个超线程，也称为逻辑核，同个物理核的多个逻辑核会共享 L1、L2缓存
* 物理核：一个 CPU 处理器中一般有多个运行核心，我们把一个运行核心称为一个物理核，每个物理核可以单独运行应用程序，一个 CPU上多个物理核共享L3缓存

在 CPU 多核场景下，Redis 实例被频繁调度到不同的 CPU 核上运行的话，那么对 Redis 实例的请求处理时间影响更大。每调度一次，一些请求就会受到
运行时信息、指令和数据重新加载过程的影响，这就会导致某些请求的延迟明显高于其他请求。

可以利用绑核 `taskset -c 0,12 ./redis-server`，将 Redis 实例绑定在 0,12 号核上(两个逻辑核位于同一个物理核上)。

但当我们把 Redis 实例绑定到一个 CPU 逻辑核上时，就会导致子进程、后台线程和Redis主线程竞争 CPU 资源，一旦子进程或者后台线程占用 CPU时，
主线程就会阻塞，导致 Redis 请求延迟增加。所以应该绑在同一个物理核上，发挥多个核心工作，同时充分利用L1、L2缓存指令数据。

#### 多 CPU 架构

为了提升 Redis 的网络性能，把操作系统的网络中断处理程序和 CPU核绑定，这样可以避免中断处理程序在不同核上来回调度执行，能够有效
提升 Redis 的网络处理性能。

在 CPU 的 NUMA 架构下，当网络中断程序和 Redis 实例各自所绑定的 CPU核 不在同一个 CPU Socket 上，那么，Redis实例读取网络数据时，
就需要跨 CPU Socket 访问内存，会花费比较多时间。

[为什么CPU结构也会影响Redis的性能？](https://time.geekbang.org/column/article/286082)


### scan

* 漏key

    Redis 在 Scan 遍历全局哈希表时，采用 __高位加一__ 的方式遍历哈希桶，当哈希表扩容后，旧哈希表中的数据映射到新哈希表，
    依旧会保留原来的顺序，保证遍历过程中不会遗漏，也不重复。
    
* 重复key

    哈希表缩容。已经遍历过的哈希桶在缩容时，会映射到新哈希表没有遍历到的位置，所以继续遍历就会对同一个key返回多次。

[美团针对Redis Rehash机制的探索和实践](https://tech.meituan.com/2018/07/27/redis-rehash-practice-optimization.html)


### 内存碎片

* 内因：内存分配器的分配策略

    内存分配器一般是按照固定的大小分配内存空间，而不是按照应用程序申请多少就分配多少，所以在分配的过程中，会偏大以便修改数据内存时可以用
    额外空间存储数据。

    Redis 使用了 jemalloc 进行分配，按照一系列固定大小的划分内存空间，比如8B、16B...8KB等，比如 Redis 申请一个20B的空间保存数据，
    那么 jemalloc 就会为其分配32B，如果再写入时，就可以利用这个空间，而不用额外分配操作。

* 外因：键值对大小不一样和删除操作

    删除操作或者修改数据（改小），导致连续的内存空间，部分出现内存碎片

#### info memory

`mem_fragmentation_ratio = used_memory_rss / used_memory`

* used_memory_rss：操作系统实际分配给 Redis 的物理内存空间  
* used_memory：Redis保存数据实际申请使用的空间
* mem_fragmentation_ratio：当前的内存碎片率，一般时 1-1.5，如果超过1.5，表明内存碎片已经超过内存的一半了

注意：如果 `mem_fragmentation_ratio` 小于1，那么分配给它的小于实际存储的，说明发生了 swap，没有足够的物理内存给 Redis，导致 Redis
一部分数据 swap 到了磁盘中，延迟增大，性能下降。

#### 清理内存碎片

`config set activedefrag yes` 开启 Redis 的自动清理策略，清理的本质是将部分数据复制到内存碎片上，并释放原来的内存空间，因为
Redis 是单线程执行，所以清理策略要考略执行的频率（清理时机）和执行的时间点（CPU）

* `active-defrag-ignore-bytes 100mb`：表示内存碎片字节数达到100MB，开始清理
* `active-defrag-thresthod-low 10`：表示内存碎片空间占系统分配给 Redis 的总空间比例达到10%，开始清理

* `active-defrag-cycle-min 25`：自动清理过程所用的 CPU 时间的比例不低于 25%，保证清理能正常工作
* `active-defrag-cycle-max 75`：自动清理过程所用 CPU 时间不高于 75%，一旦超过，停止清理，从而避免在清理时，大量拷贝内存阻塞 Redis，影响延迟

[内存占用清理](https://time.geekbang.org/column/article/289140)


### 缓冲区溢出

为了避免客户端和服务端的请求发送和处理速度不匹配，服务端给每个连接的客户端都设置了一个输入缓冲区和输出缓冲区。输入缓冲区存储客户端发送
过来的命令，等待主线程处理。处理完成之后，返回的结果将写入到输出缓冲区中，等会返回给客户端。

* 客户端输入缓冲区溢出

    1. 写入了 bigKey，默认存储大小为1G，超过嫁给你溢出
    2. 服务端处理命令请求过慢（Redis 主线程阻塞，导致无法正常处理命令，缓冲区命令堆积）
    
* 客户端输出缓冲区溢出

    1. 服务端返回 bigKey 查询的结果
    2. 执行 MONITOR 命令（持续输出监测到的命令操作，并输出到缓冲区，生产环境禁用）
    3. 缓冲区设置太小（比如大key，或者 PubSub 情况，订阅消息过多占用大量输出缓冲区）
    
    
* 复制缓冲区溢出

    全量复制过程中，主节点向从节点发送 RDB 文件的同时，会继续接受客户端的写命令，这些写命令就会保存在复制缓冲区，等 RDB 文件传输完成后，
    再发送给从节点执行，主节点会为每个从节点维护一个复制缓冲区，来保证主从节点间的数据同步。如果在这期键，从节点加载 RDB 速度慢，同时
    主节点接受大量写命令，就会发生复制缓冲器溢出。
    
    `config set client-output-buffer-limit slave 512mb 128mb 60`
    
* 复制积压缓冲区

    以上三者本质上都属于客户端缓存，位于 Redis客户端和服务端之间或者是主从之间，`client-output-buffer-limit` 一旦溢出连接将会关闭。

    `repl_blacklog_buffer` 有限的环形缓冲区，挤压缓冲区写满后，会覆盖缓冲区中的旧命令，如果从节点还没同步，那么从节点会重新开始全量复制。

    主节点在把接收到的写命令同步给从节点时，同时会把这些写命令写入复制积压缓冲区。一旦从节点发生网络闪断，再次和主节点恢复连接后， 从节点就会从复制积压缓冲区中，读取断连期间主节点接收到的写命令，进而进行增量同步


1. 命令发送过大：避免 bigKey，对于复制缓冲区来说，避免较大的 RDB 文件
2. 命令处理慢：减少 Redis 主线程上的阻塞操作，例如异步删除
3. 空间过小：合理设置缓冲区大小

其实缓冲区就是用来匹配两端发送处理请求速度不匹配的情况，而且通过buffer可以实现批量发送，减少了网络IO带来的损耗。

[缓冲区](https://time.geekbang.org/column/article/291277)

### 缓存

计算机系统中，默认有两种缓存：

* CPU 里面的末级缓存 LLC，用来存储内存中的数据，避免每次从内存中读取数据
* 内存中的高速页缓存，即 Page Cache，用来缓存磁盘中的数据，避免每次从磁盘中读取存取的数据

#### 读写缓存策略

1. 只读缓存(Cache Aside)：如果要修改数据，先去数据库中修改，然后再删除缓存中的数据，等到下个访问这个数据时，从数据库读完并写回到缓存中。优点是，数据库和缓存
   可以保证完全一直，并且缓存中永远保留的是经常访问的热点数据。缺点是每次修改操作都会把缓存中的数据删除，之后都会触发一次缓存缺失，然后从数据库加载到 缓存中，访问延迟回增大。

2. 读写缓存：同时修改数据库和缓存中的值。优点：被修改的数据永远在缓存中，下次访问时能直接命中缓存数据，性能较好，比较适合先修改然后立即访问的 业务场景，但在高并发下，存在多个操作同时修改同一个值的情况，可能会导致缓存和数据库的不一致。

3.只读缓存如果修改数据库失败，缓存中的数据也不会删除，此时数据库和缓存中的数据一致。而如果先修改缓存，然后再修改数据库，缓存成功，但数据库失败，
就会产生数据不一致。而如果先数据库，再修改缓存，会产生并发场景不一致的情况。（如果修改成功，但删除缓存失败，那么只能消息队列重试保证最终一致性
修改数据库后，将需要删除的数据存入消息队列中，如果缓存删除失败，可以通过消费者拿取消息进行缓存数据删除）

[缓存数据库一致](https://time.geekbang.org/column/article/295812)


### 雪崩、击穿、穿透

#### 缓存雪崩

缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理，进而将大量的请求转发到数据层中，导致数据库层压力激增。

1. 缓存中有大量数据同时过期，导致大量请求无法得到处理

尽量避免给大量数据设置相同的过期时间，如果一定要设置的话，给这些数据的过期时间增加一个随机值（s），避免大量数据同时过期。  
还可以通过服务降级应对雪崩（针对不同的数据采取不同的处理方式），当业务应用访问的数据非核心数据（商品属性），直接返回预定义的信息、空值或错误信息。
如果访问的是核心数据，那么仍然查询缓存，缓存缺失再查数据库（核心数据仍然会有雪崩可能，需要进一步处理）。

2. Redis 实例故障宕机

   * 在业务系统中实现服务熔断或者请求限流机制

     服务熔断：在发生缓存雪崩时，为了防止发生连锁的数据库雪崩引起整个系统的崩溃，暂停业务应用对缓存系统的接口访问，等到 Redis 实例恢复 之后，在发送请求到缓存系统（对业务影响很大）

     请求限流：限制进入业务系统中的流量请求。

   * 事前预防主从构建 Redis 缓存高可靠集群，sentinel 哨兵机制。

#### 缓存击穿

缓存击穿值某个访问非常频繁的热点数据请求，当热点数据失效时，大量请求发送给后端数据库，导致数据库压力激增。

1. 如果是非时效性的数据，那么直接缓存，不失效。
2. 如果是时效性数据，后台定时为该热点数据续约。

#### 缓存穿透

缓存穿透指访问的数据不在 Redis缓存，同时也不再数据库层，导致请求会直接穿透缓存层到达数据库层。

如果是恶意请求的，故意访问数据库不存在的数据，那么应该采用服务熔断、服务降级、请求限流的方式，对缓存和数据库层进行保护，防止大量的恶意请求把缓存 和数据库层压垮。（封禁IP、流量限制等，在网关层面）

1. 缓存空值
2. 使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据。

* 布隆过滤器

  Redis 布隆过滤器是用 string 类型实现，采用固定 bit 的数组，使用多个 hash 函数映射到多个 bit 位上，如果对应 bit 上存在都不存在，那么请求的
  数据也就不存在。如果有再去缓存层查（虽然有误判，但已经过滤了大部分无效穿透请求）

   1. 布隆过滤器会有误判，误判的本质是 hash 冲突，降低的方式可以通过增加 bit 数组的长度或者增加多个 hash 函数，但这样空间占比和计算消耗会更大，
      所以需要在误判率、空间、性能做出一个平衡。（在构建布隆过滤器时，最好预知总容量和误判率，防止后期所有bit都为1，导致的过滤时效）

   2. 因为存储的 string 是个 bigKey，所以建议单独另外部署实例，防止数据迁移时导致的 Redis 阻塞问题。

### 大实例

1. RDB快照生成：内容容量大，RDB文件大，fork的时间会增大（主线程）
2. 主从：RDB大，传输的时间也长，同时恢复的时间也比较慢，效率低
3. 缓冲区溢出：和上面的主从场景一样，RDB大，全量复制的时间长，主节点需要记录额外命令到缓冲区，时间一长就有可能造成复制缓冲区溢出


### Redis 原子操作

1. 把多个操作在 Redis 中实现成一个操作，单命令执行 `incr、decr`
2. 把多个操作写到一个 Lua 脚本中，以原子性方式执行单个 Lua 脚本。`eval`


### Redis 分布式锁

* 单机 Redis

    1. 给锁变量设置一个过期值，方式客户端宕机后，锁一直无法释放
    2. 每个客户端都应给锁变量设置唯一值，防止当前客户端因为阻塞后，锁过期其他客户端拿到锁时，阻塞恢复开始解锁
    
    加锁：`SET lock-key unique_value NX PX` 10000  
    解锁：解锁时要判断是否解锁的是当前的，避免误释放
        ```
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("dell", KEYS[1])
        else
            return 0
        end
        ```
        `redis-cli --eval unlock.script lock-key,uniquevalue`
        
    单机很难保证可靠性，就算是主从架构下，如果我们在 master 上获取了锁，但这时宕机了，锁数据还没备份到 slave 上，导致了锁丢失（锁失效）
        
* 多节点可靠分布式锁

    1. 客户端获取当前时间
    
    2. 客户端按照顺序依次向 N 个 Redis 节点实例进行加锁操作（超时失败）
    
    3. 一旦客户端完成了和所有客户端的加锁操作，客户端就要计算整个加锁过程中的耗时。
    
        客户端只有满足以下两个条件时，才能认为加锁成功：  
        
            * 条件一：客户端从超过半数（> n / 2 + 1）的 Redis 实例上成功获取到锁
            * 条件二：客户端获取锁的总耗时没有超过锁的有效时间
            
        满足这两个条件之后，重新计算锁的有效时间（最初时间-获取锁的耗时），如果条件不满足则加锁失败，那么向所有的 Redis 实例发送释放锁请求。
        
    使用 RedLock 算法获取分布式锁要避免机器时钟跳跃，否则会导致 RedLock 失效。（比如一共三个节点，A操作2个节点获取锁成功，但其中
    一个时钟发生回拨，导致A获取的其中一个节点锁失效，同时B这时获取两个锁成功，这样就造成了两个线程同时获得锁，RedLock失效）

[如果使用 Redis 分布式锁](https://time.geekbang.org/column/article/301092)        
[基于Redis的分布式锁到底安全吗（上）？](http://zhangtielei.com/posts/blog-redlock-reasoning.html)


### 主从一致

主从库之间的命令复制是异步的：

1. 如果出现网络延迟，导致复制命令迟迟无法到达从库（网络层面尽量保证主从库间的网络状况良好）
2. 即使从库正常接受到命令，但有可能因为从库正在执行复杂度高的命令（集合，聚合等），导致主线程阻塞，无法执行复制命令。

（通过一个监控程序，redis info replication 命令查询主库接受写命令进度 master_repl_offset 和从库复制写命令信息 slave_repl_offset，
如果这两个进度值的差值达到一个阈值，那么让将从库暂时移除访问，等到进度值恢复之后，再重新加回来让客户端访问）


#### 过期数据

Redis 过期数据采用的是惰性删除和定期删除。定期删除：每次会从已过期的数据集中，挑选N个进行删除，如果数据集多的话且一直没有访问到过期的
数据，那么数据将一直存留再实例中，从库仍然能读到过期数据。惰性删除：如果是在主库读，那么从库会读不到，如果是直接在从库读，并不会主动删除
，所以仍然能读到过期的数据。

Redis 3.2之后，如果访问到了过期的数据，会直接返回空。

如果使用了 `expire、pexpire`，仍然可能读到过期数据，比如从库正在进行全量复制，数据同步中，这时如果采用 `expire` 设置n个时间段后数据失效，
那么这个命令将会被记录到复制缓冲区中，等到从库完成全量复制后，开始增量复制的命令恢复，这时 expire 命令执行，导致主从两个数据的过期时间点
不一致。所以生产环境中还是更推荐 `expireat、pexpireat` 指明过期的时间戳。


### 脑裂

脑裂：主从集群中，同时有两个主节点，它们都能接受读写请求。而脑裂最直接的影响就是，客户端不知道应该往哪个主节点写入数据，结果就是不同
的客户端会往不同的主节点写入数据，进而造成进一步的数据丢失。

#### 数据丢失 - 进一步排查

1. 确认数据同步出了问题，通过对比 master_repl_offset 和 slave_repl_offset，如果 slave_repl_offset < master_repl_offset，那么
说明数据丢失是数据未同步导致的。如果新主库的 slave_repl_offset == 旧主库的 master_repl_offset，那么说明从库升级时已经和主库数据保持
一致了。下一步应该是去客户端看请求日志。

2. 排查客户端的操作日志，查看客户端中是否还有和原来主库通信的请求，特别是写请求，如果出现了，则发生了脑裂。

3. 哨兵机制进行主从切换时，超过预定数量（quorum）的哨兵实例认为主库的心跳超时，才会认为主库客观下线。接着哨兵开始选举然后执行切换操作，
当主库由于某些原因无法处理请求（比如CPU利用率暴涨，导致 Redis 主库心跳无法响应，形成了假下线），无法响应心跳，被错误认为客观下线。
结果在判断下线之后，原主库又开始处理请求，哨兵并未完成主从切换，客户端仍能正常与原主库通信。

除了主库因为阻塞无法响应心跳导致出现脑裂外，还有一种是出现网络分区，主库和客户端、哨兵和从库被分割成了两个网络，哨兵发起主从切换，
出现两个主库，客户端仍能向旧主库写入，等网络恢复之后，主库降级为从库，新主库丢失这期间的数据。

数据丢失：

主从切换之后，从库一旦升级为新主库，哨兵会让原主库执行 slave of 命令，和新主库重新进行全量同步。全量同步执行的最后阶段，原主库需要清空
本地的数据，加载新主库发来的 RDB 文件，这期间原主库的写入消息就丢失了。

应对：Redis 提供了两个配置项来限制主库的处理请求，分别为 `min-slaves-to-write` 和 `min-slaves-max-lag`

* min-slaves-to-write：设置主库能进行数据同步的最少从库数量 （从库 K/2 + 1）
* max-slaves-to-write：设置主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟（单位s，10~20）

假设为 N 和 T，配置为主库连接的从库至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则主库不会再接受客户端请求。
如果是假故障的话，无法响应哨兵心跳，同时也不能进行和从库同步，这样主库就会被限制客户端请求，也就不会写入数据。

[脑裂](https://time.geekbang.org/column/article/303568)